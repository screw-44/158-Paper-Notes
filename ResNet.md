# ResNet -- Deep Residual Learning for Image Recognition

## Abstarct
使用残差学习的网络，相比于之前unreference的学习方式。  
## 重点：
 * 对于视觉来说，深度很重要。但是做网路不是单独的堆深，梯度会发生爆炸或消失。梯度的问题可以通过更好的初始化数值和中间加入normalization layer来解决。  
 * 但是直接加深，效果会更差，训练误差还会很高，说明网络似乎收敛，但是没有到很好的结果。  

## 残差是什么？
训练的模块时残差 F(x) = H(x)-x, 所以输出H(x)=F(x)+x  
残差当输入输出不同大小，有两个方案，第一个是在周边添加0，或者增加1*1卷积，stride为2（宽高/2，通道改变）  
因为文章在backbone里面没有全连接层（1*1卷积替代），注意输出还是有fc的，所以没有dropout层了。  
加入了残差后，参数量和收敛速度都好很多  
比较不同的resnet在输入输出不一样的时候，三种不同的残差链接（A）zero-padding (B)projection 1*1卷积 （C）无论输入输出是不是一样，但是还是加入1 * 1 卷积。证明说C是最好的，到那时C带来了很大计算复杂度，所以论文用B。

***拟合曲线上，断崖式的下降是学习率乘上了0.1！！！***  

## 怎么构建更深的resnet？  
50以上的层，会引入bottleneck design。 随着层数的增加，网路可以学习到更深的特征，所以这时候会把网络的层数拉的很大。但是层数拉大会导致计算量巨大增加。为了解决这个问题，会在小的module进入是，用1 * 1卷积缩小通道数。输出时用1 * 1卷积扩大通道数。个人理解，因为本文的残差和plain的网路相对而言在module内部不需要携带所有的特征进入下一个环节，残差会帮忙把上一层的特征带到下一层。所以在残差module里面可以大幅度的缩小通道（论文中缩小了4倍）。

 **作者的想法**：一个1k层的网络，100层就学习好了，那后面900层输出就接近于1，接近于identity map。

## 为什么Resnet有用？
 **李沐**：后来人们觉得为什么resnet训练比较快，是因为他梯度保持的比较好。从梯度上来说更明显（求下偏导就可以），pain：f(g(x)), resnet:f(g(x)) + g(x)。所以resnet会提高梯度，让他能收敛到更好的地方。  
 **李沐**：为什么没有过拟合，现在还是open question