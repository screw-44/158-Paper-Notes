# Attention is all you need

## Abstarct:
在主流的序列转录模型主要依赖于复杂的循环或者是卷积神经网络，而其包含encoder和decoder的架构。Transformer只依赖于注意力机制，不需要循环或者卷积。

## Introduction
Multi-Head Attention机制，参考了卷积层能够输出多通道，表现为多个特征的特点。

## Conclusion
Transformer是第一个在序列转述的模型里面完全使用注意力机制的。把所有的循环层换成了multi-headed self-attention

